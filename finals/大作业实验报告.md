# 数据仓库与数据挖掘 期末作业报告



## （一） 分析数据集概述  

### 		数据来源

本次实验中使用的数据集来自kaggle网站上FIFA19球员数据集，原始数据集网址如下：

​														 [https://www.kaggle.com/karangadiya/fifa19](https://www.kaggle.com/karangadiya/fifa19)

Note：同时，注意到原始数据集中存在大量无关信息，我们在进行数据清洗步骤中删除了部分无用的属性（这一部分会在后面的分析流设计中进行体现）

（我们保存了清洗之后的数据集，我们将清洗之后的数据集保存到同路径文件夹下。）

### 		属性名称与属性类型

进过筛选后，我们使用pd.describe().T.to_csv("description.csv")命令,将我们的统计量输出到csv文件中。

该文件（description.csv在data📁下）为了直观，显示如下：

<img src="屏幕快照 2019-12-15 下午4.12.34.png" alt="description"  />

###  	   数据规模

原始数据集大小：(18207, 89) 。我们经过预处理后的数据集大小：(18147, 34) 

### 		数据样例

我们使用pandas中DataFrame类型内置的函数.head()来查看数据样例。

(然而并显示不下，于是采用csv文件截图方式在这里进行展示前 10条记录)

![](屏幕快照 2019-12-15 下午4.01.25.png)



## （二）分析目标  

描述数据分析预期挖掘目标，挖掘什么样的模型。

1.  根据球员各项能力值对球员的场上作用进行分类。
    -   我们知道在足球队中，可以将每一个球员粗略的分成以下几类：
        -   门将 ("goalkeeper") : 0  	后场 ("defender") : 1 中场("midfielder") : 2 	前锋("forward") : 3
    -   本此实验中，我们使用神经网络（python）对比SPSS中分类模型，利用数据集中提供的个球员能力值对球员场上作用（位置）进行预测。
2.  球员能力值聚类任务。
    -   利用我们原始数据集中各个球员的能力值，我们尝试才用无监督的方式将球员进行聚类。
    -   利用（sklearn 库中 KMeans模型）对比SPSS中的聚类节点，对球员能力进行聚类。
    -   利用PCA将球员能力值进行降维，通过（matplotlib）作图对于聚类结果进行可视化。
3.  球员年龄和球员潜力值的关联规则
    -   从直觉上来说，对于一个年轻球员，其往往有更大的上升空间，本实验中研究这种“Association”是否真正成立
    -   利用SPSS对于关联规则进行分析

## （三）分析流设计  & Python 数据分析设计过程

 注：说明modeler预处理、建模步骤，每个步骤计算用什么节点做什么工作。

### 	task 1 球员位置分类任务

#### 			Python 数据分析设计过程

-   数据集预处理：

    -   首先先删除球员位置属性为空的记录。（**<u>处理空值的方法</u>**）

    -   然后，我们根据原始数据中postion属性内容为每条球员记录增加一个duty属性，数值类型属性。

        duty属性分成四类：门将 ("goalkeeper") : 0  	后场 ("defender") : 1 

        ​									中场("midfielder") : 2 	前锋("forward") : 3

        postion， duty对应关系如下所示：

        | Postion                                                      | Duty |
        | ------------------------------------------------------------ | ---- |
        | "ST", "LW", "RW", "LF", "RF", "RS","LS", "CF"                | 3    |
        | "CM","RCM","LCM", "CDM","RDM","LDM", "CAM", "LAM", "RAM", "RM", "LM" | 2    |
        | "CB", "RCB", "LCB", "LWB", "RWB", "LB", "RB"                 | 1    |
        | “GK”                                                         | 0    |

    -   之后，我们从原始数据中晒选出球员能力值的数据作为我们模型的输入。球员能力值详细解释表（见附录（一））：

    -   然后利用sklearn中 sklearn.preprocessing StandardScaler 对数据进行标准化。
    -   利用keras.utils.np_utils to_categorical函数将数值类型的target转换为one-hot编码，方便之后使用内置的categorical_crossentropy损失函数。
    -   最后利用sklearn.model_selection train_test_split函数划分训练集和测试集。

-   使用传统深度学习模型 ：linear层建模，最后的输出使用softmax作为激活函数，其余层使用relu激活，在最后一层之前增加dropout防止过拟合。得到我们实验中使用的模型。

-   模型训练 & 测试

    -   测试结果使用confuse_matrix进行统计，并进行简单的可视化工作。

        #### SPSS流设计

-   由于我们已经用python进行了数据预处理工作， 我们直接使用使用python数据预处理后的数据进行分析。
-   我们首先使用变量文件节点将预处理好的文件进行导入

![](屏幕快照 2019-12-15 下午6.16.32.png)


-   使用过滤器将id编号去掉

![](屏幕快照 2019-12-15 下午6.21.49.png)

-   使用样本节点划分数据集为训练集和测试集

![](屏幕快照 2019-12-15 下午6.22.29.png)

-   使用类型节点将需要预测的Duty属性设置为目标，类型设置为标记类型。

![](屏幕快照 2019-12-15 下午6.23.08.png)

-   使用C5.0 决策树模型完成多分类任务。

-   使用分析节点对模型能力进行分析

![](屏幕快照 2019-12-15 下午6.23.49.png)

### 	task 2 球员能力值聚类任务

#### 		Python 数据分析设计过程

-   数据集预处理：
    -   这里的数据预处理过程和上面的task 1中的数据预处理比较类似，由于不需要考虑球员位置同时考虑到任务本身为无监督任务，我们这里只需要在筛选能力值数据后，对数据进行数据标准化即可。
-   使用K-means模型：将能力值分成四类（仍然想体现球员在不同位置上作用应当有显著差异）。
-   模型训练 ：
    -   具体模型参数： 
    -   模型损失值：
-   模型效果展示：并利用PCA将球员能力值降到二维，对我们的K-means模型结果进行可视化



#### 	  SPSS流设计

-   同实验一，我们首先使用变量文件节点将预处理好的文件进行导入

![](屏幕快照 2019-12-15 下午7.19.09.png)

-   使用类型节点将ID，Duty表项角色设置为无，其余表项角色为输入，并将Duty表项定义为名义属性。

![](屏幕快照 2019-12-15 下午7.21.40.png)

-   使用K-Means节点将聚类数目设定为4进行训练

![](屏幕快照 2019-12-15 下午7.25.11.png)

-   将结果按照Duty属性，和预测聚类类型排序

![](屏幕快照 2019-12-15 下午7.25.29.png)

-   使用矩阵节点观察confuse matrix

    ![](屏幕快照 2019-12-15 下午7.25.36-6409199.png)

    

    ### task 3 球员年龄与球员潜力值关联规则分析

（略）

## （四）分析流实现  

​         注：说明modeler分析流的具体内容：数据预处理、数据挖掘建模、模型结果、模型评估效果分析；要求流根据截图进行说明，流可分为多个文件；也可分为多个流进行分析  

### 	Task one 分类任务

#### 		Python 神经网络版

##### 数据预处理

-   Step one : filter out the record without 'Postion' attributes

```python
player_data = pd.read_csv("../input/fifa19/data.csv", header=0,  na_values=['.', '??','?', '', ' ', 'NA', 'na', 'Na', 'N/A', 'N/a', 'n/a'])

legal_index = pd.notnull(player_data['Position'])

player_data = player_data.loc[legal_index]

player_data.index = range(len(player_data))
```

-   Step two : add Duty attribute

```python
forward = ["ST", "LW", "RW", "LF", "RF", "RS","LS", "CF"]
midfielder = ["CM","RCM","LCM", "CDM","RDM","LDM", "CAM", "LAM", "RAM", "RM", "LM"]
defender = ["CB", "RCB", "LCB", "LWB", "RWB", "LB", "RB"]

player_data.loc[player_data["Position"] == "GK", "Duty"] = 0
player_data.loc[player_data["Position"].isin(defender), "Duty"] = 1
player_data.loc[player_data["Position"].isin(midfielder), "Duty"] = 2
player_data.loc[player_data["Position"].isin(forward), "Duty"] = 3

player_data['Duty'] = player_data['Duty'].astype('int')
```

-   Step three : filter out columns without ability info.

```python
ability = ['Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling',
       'Curve', 'FKAccuracy', 'LongPassing', 'BallControl', 'Acceleration',
       'SprintSpeed', 'Agility', 'Reactions', 'Balance', 'ShotPower',
       'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression',
       'Interceptions', 'Positioning', 'Vision', 'Penalties', 'Composure',
       'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling',
       'GKKicking', 'GKPositioning', 'GKReflexes', 'Duty']

player_data_filtered = player_data[ability]
```

-   Step four : fill the ability attribute of null with the mean_value of the column. 

```python
# but in fact our dataset is complete enough with no null values now ! so there is no need to fill the null values.
col_with_null = [col for col in player_data_filtered.columns
                if player_data_filtered[col].isnull().any()]

print(col_with_null)
```

-   Step five : save our filtered data

```python
player_data_filtered.to_csv('/kaggle/working/filtered_data.csv')
```

-   Step Six : get the description

```python
print(player_data_filtered.shape)

player_data_filtered.describe().T.to_csv('/kaggle/working/description.csv')
```

-   Step Seven : standardlize the attributes of players.

```python
X = player_data_filtered.drop("Duty", axis = 1)

from sklearn.preprocessing import StandardScaler

Scaler = StandardScaler()

X = Scaler.fit_transform(X)

Y = player_data_filtered['Duty']
```

-   Step Eight : split our traininng and test set.

```python
x_train = X[:14000]; y_train = Y[:14000]

x_test = X[14000:]; y_test = Y[14000:]
y_test.index = range(len(x_test))

# Step two : construct our dataset.
import torch.utils.data as data
import torch
class Player_Dataset(data.Dataset):
    def __init__(self, x, y):
        self.input = x
        self.target = y
    def __getitem__(self, index):
        return self.input[index], self.target[inex]
    def  __len__(self):
        return len(self.target)
    
train_dataset = Player_Dataset(x_train, y_train)

test_dataset = Player_Dataset(x_test, y_test)

# Step Three : construct our dataloader.
train_iter = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=4)

test_iter = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=True, num_workers=4)
```

##### 搭建神经网络模型

-   Step one : build our model 

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class Classifier(nn.Module):
    def __init__(self, input_size, output_size, dropout):
        super(Classifier, self).__init__()
        self.dropout = dropout
        self.output_size = output_size
        self.input_size = input_size
        
        self.linear_1 = nn.Linear(input_size, 128)
        self.linear_2 = nn.Linear(128, 128)
        self.dropout = nn.Dropout(dropout)
        self.linear_3 = nn.Linear(128, output_size)
        
    def forward(self, input):
        temp_result = F.relu(self.linear_1(input))
        temp_result = F.relu(self.linear_2(temp_result))
        temp_result = self.dropout(temp_result)
        result = F.softmax(self.linear_3(temp_result), dim=1)
        return result
    
model = Classifier(33, 4, 0.5).to(device)

```

-   Step two : define our optimizer and criterion

```python
optimizer = optim.Adam(model.parameters())

criterion = nn.CrossEntropyLoss().to(device)
```

-   Step Three : train our model

```python
def train(model, data, optimizer, criterion):
    model.train()
    epoch_loss = 0
    for i, batch in enumerate(data):
        Input, target = batch
        Input = Input.float().to(device); target = target.to(device)
        output = model(Input)
        loss = criterion(output, target)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        epoch_loss += loss.item()
        
    return epoch_loss / len(data)
```

```python
N_epoches = 10
best_valid_loss = float('inf')

for epoch in range(N_epoches):
    
    start_time = time.time()
    
    train_loss = train(model, train_iter, optimizer, criterion)
    
    end_time = time.time()
    
    epoch_mins, epoch_secs = epoch_time(start_time, end_time)
    
    if train_loss < best_valid_loss:
        best_valid_loss = train_loss
        torch.save(model.state_dict(), 'model.pt')
        
    print(f'Epoch:  {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')
    print(f'\tTrain  Loss: {train_loss: .3f}')
```

#### 		SPSS 流实现

![](屏幕快照 2019-12-15 下午6.14.29.png)


### 	Task two 聚类任务

#### 		Python 神经网络版

-   Build our experiment model using sklearn.

```python
from sklearn.cluster import KMeans
from sklearn import metrics

# using sklearn create KMeans model to do our experiments.
KMean_model = KMeans(n_clusters=4, random_state=9)
y_pred = KMean_model.fit_predict(X)
```

-   可以看出使用sklearn 的机器学习方法就是这样简单直接，短短4行代码将KMeans聚类方法展现给了使用者。

    #### 	SPSS流实现

![](屏幕快照 2019-12-15 下午7.06.06.png)

-   这里补充一下：在本次实验中，我使用了两种完全不同的机器学习/深度学习库，pytorch & sklearn。
    -   简单来说，sklearn封装了几乎所有传统机器学习方法，我们需要做的就是定义模型，构造数据集，fit+predict 完成我们对于未知值的预测。
        -   这种简单粗暴的方法适合大部分典型场景应用，比较适合做商业开发使用，效果不错。
        -   然而这种方法对于更加复杂的模型效果有一定局限，可以做baseline，但不会是STOA。
        -   Note： sklearn 貌似不支持GPU加速，这也是另一个问题
    -   对于pytorch，这是一个新兴的深度学习框架，本人在初次接触的时候确实感觉比较晦涩难懂。
        -   这种方法明显代码量远大于sklearn/keras。
        -   这种方法的可调式性更强大，较于keras/sklearn 能力更强，使用于更加广泛的场景。

## （五）数据分析总结

### 	Task one 分类任务

#### 		Python 结果分析

这个问题是一个典型的四分类任务：将球员按场上位置分成四类。

随机分类的准确度应该不超过30%, 我们使用分类正确所占比对于模型的效果进行测试：

多次实验表明： 我们的模型精确度在87.5%-88.5%之间 （验证模型精确度代码如下：）

```python
from sklearn.metrics import classification_report

def evaluate():
    result_on_test = []; ground_truth = []
    model.eval()
    for i, batch in enumerate(test_iter):
        features, target = batch
        ground_truth.extend(list(target))
        with torch.no_grad():
            output = model(features.float())
            output = np.argmax(output, axis=1)
            result_on_test.extend(output)
            
    acc = sum([(ground_truth[i] == result_on_test[i]).item() for i in range(len(result_on_test))]) / len(result_on_test)
    print(f"The classification accuracy is : {acc*100:.2f} %\n")   
    print(classification_report(ground_truth, result_on_test))
```

模型一次测试结果：

<img src="屏幕快照 2019-12-15 下午4.27.18.png" style=
"zoom:50%;" />

模型效果分析：总体来说模型对于四种类型均能达到较好的效果，模型区分守门员（0）的能力尤其之强。

#### 		SPSS 结果分析

![](屏幕快照 2019-12-15 下午6.10.33.png)

#### 	Python & SPSS结果对比

-   Python 模型在测试集上的准确度较SPSS准确度高9%左右。

    ### Task two 聚类任务

    #### 	Python 结果分析			

我们使用KMeans将球员的各个能力值（除去duty列）进行聚类（四类）：

直觉上，我们仍然认为聚类任务的效果仍应该和之前对于球员不同作用的分类有相似之处（不同位置的球员应当有不同的能力特征，相同位置的球员应该有相似的能力特征）。

基于此，我们仍使用Duty作为一项标准进行比较。

-   首先使用PCA将球员33中能力值进行压缩，压缩到2维，并对其使用matplotlib进行可视化。

```python
from sklearn.decomposition import PCA
from sklearn import metrics
%pylab inline

pca = PCA(n_components=2)
X_decomposition = pca.fit_transform(X)
fig = plt.figure(figsize=(5, 5))
plt.scatter(X_decomposition[:, 0], X_decomposition[:,1], c = y_pred,
                 cmap = plt.cm.get_cmap("tab10", 10), alpha = 0.5)
```

<img src="屏幕快照 2019-12-15 下午4.36.35.png"  />

通过可视化，我们可以发现我们的聚类效果不错。

同时猜测粉色应该与守门员类型对应，其余位置在压缩后没有很明显的边界。

-   其次我们手写了一个优雅的绘制confuse_matrix的函数。检验了聚类效果

```python
from sklearn.metrics import confusion_matrix
import seaborn as sns

def plot_confusion_matrix(y_true, y_pred, classes,
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    # Only use the labels that appear in the data
    classes = classes
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    fig, ax = plt.subplots(figsize=(10,10))    

    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")

    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax

# print(classification_report(list(Y), y_pred))

plot_confusion_matrix(list(Y), y_pred, classes=np.array(["0","1","2","3"]), title='Confusion matrix using KMeans')
```

<img src="屏幕快照 2019-12-15 下午4.36.50.png" style="zoom:50%;" />

可以看到总体来说，守门员均被分成了一个聚类，守门员确实是足球场上的“特殊”人才。

#### 		SPSS结果分析

-   首先我们的聚类质量处于良好水平：从右边的饼状图一可以看到各个类别的占比分布：

![](屏幕快照 2019-12-15 下午7.08.18.png)

-   我们进一步想要知道那些能力值是区分球员位置的核心：

![](屏幕快照 2019-12-15 下午7.09.09.png)从上面图片中，我们可以看到，Acceleratoin, Aggression, Agility 是区分球员位置的重要因素。不同位置球员这三项能力值区别较大。

-   除此之外，我们还希望知道这样的聚类和我们球员的位置（前锋，中场，后卫，门将）间的关联：

![](屏幕快照 2019-12-15 下午7.13.14.png)

SPSS中用confuse matrix分析得到了和Python中使用sklearn.cluster.KMeans相似的结果。

## （六）分工说明  

​        注：本实验仅有实验者本人独自完成。

## （七）参考文献  

### 使用到的资料汇总

1.  线性模型参考本人之前做过的boston房价模型：[https://www.kaggle.com/columbine/boston-house-fnn-model](https://www.kaggle.com/columbine/boston-house-fnn-model)
2.  关于数据集的构建方法参考本人的image caption notebook

[https://www.kaggle.com/columbine/final-project-image-caption](https://www.kaggle.com/columbine/final-project-image-caption)

3.  关于keras参考keras官方文档[https://keras.io/zh/](https://keras.io/zh/)
4.  算法方面关联规则原理参考 数据仓库与数据挖掘课程PPT

### 致谢

在这里再次感谢在本人完成本次试验过程中帮助过我的老师和同学。感谢数据仓库与数据挖掘课程提供这样一次不仅仅局限于课本内容，并能动手操作的机会。

## （八）附录

### 	（一） 球员属性对应关系表

| Attribute          | Explanation                      |
| ------------------ | -------------------------------- |
| finishing          | 完成射门（能力值）。数值变量。   |
| heading_accuracy   | 头球精度（能力值）。数值变量。   |
| short_passing      | 短传（能力值）。数值变量。       |
| volleys            | 凌空球（能力值）。数值变量。     |
| dribbling          | 盘带（能力值）。数值变量。       |
| curve              | 弧线（能力值）。数值变量。       |
| free_kick_accuracy | 定位球精度（能力值）。数值变量。 |
| long_passing       | 长传（能力值）。数值变量。       |
| ball_control       | 控球（能力值）。数值变量。       |

| Attribute    | Explanation                    |
| ------------ | ------------------------------ |
| acceleration | 加速度（能力值）。数值变量。   |
| sprint_speed | 冲刺速度（能力值）。数值变量。 |
| agility      | 灵活性（能力值）。数值变量。   |
| reactions    | 反应（能力值）。数值变量。     |
| balance      | 身体协调（能力值）。数值变量。 |
| shot_power   | 射门力量（能力值）。数值变量。 |
| jumping      | 弹跳（能力值）。数值变量。     |
| stamina      | 体能（能力值）。数值变量。     |
| strength     | 力量（能力值）。数值变量。     |

| Attribute       | Explanation                  |
| --------------- | ---------------------------- |
| long_shots      | 远射（能力值）。数值变量。   |
| aggression      | 侵略性（能力值）。数值变量。 |
| interceptions   | 拦截（能力值）。数值变量。   |
| positioning     | 位置感（能力值）。数值变量。 |
| vision          | 视野（能力值）。数值变量。   |
| penalties       | 罚点球（能力值）。数值变量。 |
| marking         | 卡位（能力值）。数值变量。   |
| standing_tackle | 断球（能力值）。数值变量。   |
| sliding_tackle  | 铲球（能力值）。数值变量。   |

| Attribute      | Explanation                      |
| -------------- | -------------------------------- |
| gk_diving      | 门将扑救（能力值）。数值变量。   |
| gk_handling    | 门将控球（能力值）。数值变量。   |
| gk_kicking     | 门将开球（能力值）。数值变量。   |
| gk_positioning | 门将位置感（能力值）。数值变量。 |
| gk_reflexes    | 门将反应（能力值）。数值变量。   |

### 附录（二）本实验中使用的python代码

在notebook📁中，文件格式为.ipynb格式，而不是.py格式，打开时需要使用jupyter notebook工具。

当然，本实验的notebook已经在kaggle上公开，连接如下[https://www.kaggle.com/columbine/data-mining-final-project](https://www.kaggle.com/columbine/data-mining-final-project) 欢迎大家在kaggle上views，votes ！