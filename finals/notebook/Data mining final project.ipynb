{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare the data\n\n\n1. fliter out the record without 'Position' attributes.\n\n2. add and duty attribute contain 4 kind of value : (0: \"goalkeeper\", 1: \"defender\", 2: \"midfielder\", 3: \"forward\")\n\n3. filter out columns without ability info.\n\n4. fill the null value in the ability table with the mean value in the table."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Step one : filter out the record without 'Postion' attributes\nplayer_data = pd.read_csv(\"../input/fifa19/data.csv\", header=0,  na_values=['.', '??','?', '', ' ', 'NA', 'na', 'Na', 'N/A', 'N/a', 'n/a'])\n\nlegal_index = pd.notnull(player_data['Position'])\n\nplayer_data = player_data.loc[legal_index]\n\nplayer_data.index = range(len(player_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"player_data.hist(column=['HeadingAccuracy', 'Finishing'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step two : add Duty attribute\n\nforward = [\"ST\", \"LW\", \"RW\", \"LF\", \"RF\", \"RS\",\"LS\", \"CF\"]\nmidfielder = [\"CM\",\"RCM\",\"LCM\", \"CDM\",\"RDM\",\"LDM\", \"CAM\", \"LAM\", \"RAM\", \"RM\", \"LM\"]\ndefender = [\"CB\", \"RCB\", \"LCB\", \"LWB\", \"RWB\", \"LB\", \"RB\"]\n\nplayer_data.loc[player_data[\"Position\"] == \"GK\", \"Duty\"] = 0\nplayer_data.loc[player_data[\"Position\"].isin(defender), \"Duty\"] = 1\nplayer_data.loc[player_data[\"Position\"].isin(midfielder), \"Duty\"] = 2\nplayer_data.loc[player_data[\"Position\"].isin(forward), \"Duty\"] = 3\n\nplayer_data['Duty'] = player_data['Duty'].astype('int')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Is Finishing a important skill for a forward or defender or midfielder?\n\n1. get a sub table of all forward.\n\n2. using sns to plot a regplot."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n%pylab inline\nimport random\n# get a subtable\nplayer_data_forward =  player_data.loc[player_data['Duty']==3]\nplayer_data_midfielder = player_data.loc[player_data['Duty']==2]\nplayer_data_defender = player_data.loc[player_data['Duty']==1]\n\nindex = [i for i in range(player_data_forward.shape[0])]\nrandom.shuffle(index)\nplayer_data_forward = player_data_forward.iloc[index]\nplayer_data_midfielder = player_data_midfielder.iloc[index]\nplayer_data_defender = player_data_defender.iloc[index]\nsns.set_style('whitegrid')\nsns.regplot(x='Overall', y='Finishing', data=player_data_forward[:100])\nplt.figure()\nsns.regplot(x='Overall', y='Finishing', data=player_data_midfielder[:100])\nplt.figure()\nsns.regplot(x='Overall', y='Finishing', data=player_data_defender[:100])\nplt.figure()\ndel player_data_forward, player_data_midfielder, player_data_defender","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step three : filter out columns without ability info.\nability = ['Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling',\n       'Curve', 'FKAccuracy', 'LongPassing', 'BallControl', 'Acceleration',\n       'SprintSpeed', 'Agility', 'Reactions', 'Balance', 'ShotPower',\n       'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression',\n       'Interceptions', 'Positioning', 'Vision', 'Penalties', 'Composure',\n       'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling',\n       'GKKicking', 'GKPositioning', 'GKReflexes', 'Duty']\n\nplayer_data_filtered = player_data[ability]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill the ability attribute of null with the mean_value of the column. \n# but in fact our dataset is complete enough with no null values now ! so there is no need to fill the null values.\ncol_with_null = [col for col in player_data_filtered.columns\n                if player_data_filtered[col].isnull().any()]\n\nprint(col_with_null)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"player_data_filtered.to_csv('/kaggle/working/filtered_data.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Statistics about the filtered dataset.\n\n1. how many records & how many attributes are there in our filtered dataset.\n\n2. some statistic about each column."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(player_data_filtered.shape)\n\nplayer_data_filtered.describe().T.to_csv('/kaggle/working/description.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# standardlize the attributes of players.\n\nX = player_data_filtered.drop(\"Duty\", axis = 1)\n\nfrom sklearn.preprocessing import StandardScaler\n\nScaler = StandardScaler()\n\nX = Scaler.fit_transform(X)\n\nY = player_data_filtered['Duty']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Step one :split our traininng and test set.\nx_train = X[:14000]; y_train = Y[:14000]\n\nx_test = X[14000:]; y_test = Y[14000:]\ny_test.index = range(len(x_test))\n\n# Step two : construct our dataset.\nimport torch.utils.data as data\nimport torch\nclass Player_Dataset(data.Dataset):\n    def __init__(self, x, y):\n        self.input = x\n        self.target = y\n    def __getitem__(self, index):\n        return self.input[index], self.target[index]\n    def  __len__(self):\n        return len(self.target)\n    \ntrain_dataset = Player_Dataset(x_train, y_train)\n\ntest_dataset = Player_Dataset(x_test, y_test)\n\n# Step Three : construct our dataloader.\ntrain_iter = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=4)\n\ntest_iter = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=True, num_workers=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Our Model\n\nFirst Task is to use a Neural Network module to do a claasification on Duty according to the attributes of the player.\n\nThe Second Task is to build a K-Means model to cluster the player ability attribute.\n\nThe last task is to consider the Association Rules between Player's Age and Player's Potential."},{"metadata":{},"cell_type":"markdown","source":"### The first Task \n\nUsing pytorch to build a nerual network classifier to predict the player's duty.\n\n1. build our model \n\n2. define our optimizer and criterion\n\n3. train our model\n\n4. evaluate our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass Classifier(nn.Module):\n    def __init__(self, input_size, output_size, dropout):\n        super(Classifier, self).__init__()\n        \n        self.dropout = dropout\n        self.output_size = output_size\n        self.input_size = input_size\n        \n        self.linear_1 = nn.Linear(input_size, 128)\n        self.linear_2 = nn.Linear(128, 128)\n        self.dropout = nn.Dropout(dropout)\n        self.linear_3 = nn.Linear(128, output_size)\n        \n    def forward(self, input):\n#         print(input.shape)\n        temp_result = F.relu(self.linear_1(input))\n#         print(temp_result.shape)\n        temp_result = F.relu(self.linear_2(temp_result))\n#         print(temp_result.shape)\n        temp_result = self.dropout(temp_result)\n#         print(temp_result.shape)\n        result = F.softmax(self.linear_3(temp_result), dim=1)\n#         print(result.shape)\n        return result\n    \nmodel = Classifier(33, 4, 0.5).to(device)\nprint(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define our optimizer and  criterion.\n\noptimizer = optim.Adam(model.parameters())\n\ncriterion = nn.CrossEntropyLoss().to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, data, optimizer, criterion):\n    model.train()\n    epoch_loss = 0\n    for i, batch in enumerate(data):\n        Input, target = batch\n        Input = Input.float().to(device); target = target.to(device)\n\n        output = model(Input)\n#         print(\"output\", output)\n#         print(\"target\", target)\n        loss = criterion(output, target)\n#         print(loss)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        \n    return epoch_loss / len(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time  / 60)\n    elapsed_secs = int(elapsed_time -  (elapsed_mins * 60))\n    return  elapsed_mins, elapsed_secs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_epoches = 10\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_epoches):\n    \n    start_time = time.time()\n    \n    train_loss = train(model, train_iter, optimizer, criterion)\n    \n    end_time = time.time()\n    \n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if train_loss < best_valid_loss:\n        best_valid_loss = train_loss\n        torch.save(model.state_dict(), 'model.pt')\n        \n    print(f'Epoch:  {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain  Loss: {train_loss: .3f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\ndef evaluate():\n    result_on_test = []; ground_truth = []\n    model.eval()\n    for i, batch in enumerate(test_iter):\n        features, target = batch\n        ground_truth.extend(list(target))\n        with torch.no_grad():\n            output = model(features.float())\n            output = np.argmax(output, axis=1)\n            result_on_test.extend(output)\n            \n    acc = sum([(ground_truth[i] == result_on_test[i]).item() for i in range(len(result_on_test))]) / len(result_on_test)\n    print(f\"The classification accuracy is : {acc*100:.2f} %\\n\")   \n    print(classification_report(ground_truth, result_on_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also write a Decision Tree model using sklearn. So now Let's begin!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\n\ntree_clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n# to record time we used.\n\ntree_clf = tree_clf.fit(x_train, y_train)\n\ny_test_pred = tree_clf.predict(x_test)\n\nscore_decisionTree = metrics.accuracy_score(y_test, y_test_pred)\n\nprint(score_decisionTree)\nprint(classification_report(y_test, y_test_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%pylab inline\ndef fine_tuning():\n    result = []\n    for i in range(3, 11, 1):\n        tree_clf_tmp = DecisionTreeClassifier(criterion=\"entropy\", \n                                                     max_depth=i)\n        tree_clf_tmp = tree_clf_tmp.fit(x_train, y_train)\n        y_pred_tmp = tree_clf_tmp.predict(x_test)\n        score_decisionTree_tmp = metrics.accuracy_score(y_test, y_pred_tmp)\n        result.append(score_decisionTree_tmp)\n    result = np.array(result)\n    fig = plt.figure(figsize=(8,8))\n    plt.plot(range(3, 11, 1), result, c='blue')\n    plt.xlabel(r\"max_depth\")\n    plt.ylabel(r'accuracy')\n    plt.show()\n    \nfine_tuning()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pydotplus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import export_graphviz\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nimport pydotplus\n\ndot_data = StringIO()\nexport_graphviz(tree_clf, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True,\n                class_names=['0','1','2','3'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('DecisionTree_Model.png')\nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task two \n\nwe are going to build a K-Means model to do the cluster task on player's attribute.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn import metrics\n\n# using sklearn create KMeans model to do our experiments.\nKMean_model = KMeans(n_clusters=4, random_state=9)\ny_pred = KMean_model.fit_predict(X)\ny_pred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn import metrics\n%pylab inline\n\npca = PCA(n_components=2)\nX_decomposition = pca.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(5, 5))\n\nplt.scatter(X_decomposition[:, 0], X_decomposition[:,1], c = y_pred,\n                 cmap = plt.cm.get_cmap(\"tab10\", 10), alpha = 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.calinski_harabasz_score(X, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(Y[10:20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    fig, ax = plt.subplots(figsize=(10,10))    \n\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\n\n# print(classification_report(list(Y), y_pred))\n\nplot_confusion_matrix(list(Y), y_pred, classes=np.array([\"0\",\"1\",\"2\",\"3\"]), title='Confusion matrix using KMeans')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 球员能力值的关联规则分析\n\n-   我们知道球员能力值之间并不是相互独立的，比如一个球员的Vision（视野）直接影响了球员的LongPassing（长传）能力\n\n-   本次实验中，我们想要验证这种球员能力值之间的关联规则。主要考虑一种关联规则：\n\n    -   球员ShotPower和Finishing之间的关联规则\n    \n- 我们使用经过过滤得到的数据进行分析"},{"metadata":{"trusted":true},"cell_type":"code","source":"# using pandas built-in plot method to verify our assumption.\n%pylab inline\n\nplt.rcParams['figure.figsize'] = (10.0, 8.0) \nplt.rcParams['image.interpolation'] = 'nearest' # 设置 interpolation style\nplt.rcParams['image.cmap'] = 'gray'  \n\n# sns.set_style('whitegrid')\nsns.regplot(x='ShotPower', y='Finishing', data=player_data_filtered[:250])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_discrete():\n    player_data_filtered_discrete = player_data_filtered.copy()\n    for column in player_data_filtered_discrete.columns:\n        if column == \"Duty\":\n            continue\n        player_data_filtered_discrete[column] = pd.cut(\n            player_data_filtered_discrete[column], [0, 30, 60, 100], labels=['bottom', 'middle', 'top'])\n    return player_data_filtered_discrete\n\nplayer_data_filter_discrete = transform_discrete()\nplayer_data_filter_discrete.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"player_data_filter_discrete.to_csv('/kaggle/working/player_data_filter_discrete.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Finishing = player_data_filter_discrete['Finishing'].groupby(player_data_filter_discrete['Finishing']).count()\nShotPower = player_data_filter_discrete['ShotPower'].groupby(player_data_filter_discrete['ShotPower']).count()\n\nplt.rcParams['figure.figsize'] = (10.0, 8.0) \nplt.rcParams['image.cmap'] = 'gray' \n# show first 28 picture of the data after filter\nfig, axes = plt.subplots(1, 2, figsize = (14, 7))\n\nplt.subplots_adjust(hspace=0, wspace=0)\n\naxes[0].set_title('Finishing Distribution')\naxes[0].pie(x=Finishing, labels=['bottom', 'medium', 'top'])\naxes[0].set(xticks=[], yticks = [])\naxes[1].set_title('ShotPower Distribution')\naxes[1].pie(x=ShotPower, labels=['bottom', 'medium', 'top'])\naxes[1].set(xticks=[], yticks = [])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_Finishing = player_data_filter_discrete['Finishing'].loc[player_data_filter_discrete['Finishing']=='top']\nbottom_Finishing = player_data_filter_discrete['Finishing'].loc[player_data_filter_discrete['Finishing']=='bottom']\n\ntop_ShotPower = player_data_filter_discrete['ShotPower'].loc[player_data_filter_discrete['ShotPower']=='top']\nbottom_ShotPower = player_data_filter_discrete['ShotPower'].loc[player_data_filter_discrete['ShotPower']=='bottom']\n\nboth_top = player_data_filter_discrete.loc[(player_data_filter_discrete['Finishing']=='top') & (player_data_filter_discrete['ShotPower']=='top')]\nboth_bottom = player_data_filter_discrete.loc[(player_data_filter_discrete['Finishing']=='bottom') & (player_data_filter_discrete['ShotPower']=='bottom')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"P_bothTop = both_top.shape[0] / player_data_filter_discrete.shape[0]\n\nP_topFinishing = top_Finishing.shape[0] / player_data_filter_discrete.shape[0]\n\nP_topShotPower = top_ShotPower.shape[0] / player_data_filter_discrete.shape[0]\n\nprint(f\"Support rate: {P_bothTop*100:.2f}% \\n,\\\nConfidence (top shot power => top Finishing) : {P_bothTop / P_topShotPower*100:.2f}%\\n,\\\nConfidence (top Finishing => top shot power) : {P_bothTop / P_topFinishing*100:.2f}%\\n,\\\nLift rate : {P_bothTop / (P_topFinishing*P_topShotPower) * 100:.2f}%\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"P_bothBottom = both_bottom.shape[0] / player_data_filter_discrete.shape[0]\n\nP_bottomFinishing = bottom_Finishing.shape[0] / player_data_filter_discrete.shape[0]\n\nP_bottomShotPower = bottom_ShotPower.shape[0] / player_data_filter_discrete.shape[0]\n\nprint(f\"Support rate: {P_bothBottom*100:.2f}% \\n,\\\nConfidence (bottom shot power => bottom Finishing) : {P_bothBottom / P_bottomShotPower*100:.2f}%\\n,\\\nConfidence (bottom Finishing => bottom shot power) : {P_bothBottom / P_bottomFinishing*100:.2f}%\\n,\\\nLift rate : {P_bothBottom / (P_bottomFinishing*P_bottomShotPower) * 100:.2f}%\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}