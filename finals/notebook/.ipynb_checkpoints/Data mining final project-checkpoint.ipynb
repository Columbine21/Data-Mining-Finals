{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "\n",
    "1. fliter out the record without 'Position' attributes.\n",
    "\n",
    "2. add and duty attribute contain 4 kind of value : (0: \"goalkeeper\", 1: \"defender\", 2: \"midfielder\", 3: \"forward\")\n",
    "\n",
    "3. filter out columns without ability info.\n",
    "\n",
    "4. fill the null value in the ability table with the mean value in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Step one : filter out the record without 'Postion' attributes\n",
    "# player_data = pd.read_csv(\"../input/fifa19/data.csv\", header=0,  na_values=['.', '??','?', '', ' ', 'NA', 'na', 'Na', 'N/A', 'N/a', 'n/a'])\n",
    "player_data = pd.read_csv(\"../data/data.csv\", header=0, \n",
    "                         na_values=['.', '??','?', '', ' ', 'NA', 'na', 'Na', 'N/A', 'N/a', 'n/a'])\n",
    "\n",
    "legal_index = pd.notnull(player_data['Position'])\n",
    "\n",
    "player_data = player_data.loc[legal_index]\n",
    "\n",
    "player_data.index = range(len(player_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step two : add Duty attribute\n",
    "\n",
    "forward = [\"ST\", \"LW\", \"RW\", \"LF\", \"RF\", \"RS\",\"LS\", \"CF\"]\n",
    "midfielder = [\"CM\",\"RCM\",\"LCM\", \"CDM\",\"RDM\",\"LDM\", \"CAM\", \"LAM\", \"RAM\", \"RM\", \"LM\"]\n",
    "defender = [\"CB\", \"RCB\", \"LCB\", \"LWB\", \"RWB\", \"LB\", \"RB\"]\n",
    "\n",
    "player_data.loc[player_data[\"Position\"] == \"GK\", \"Duty\"] = 0\n",
    "player_data.loc[player_data[\"Position\"].isin(defender), \"Duty\"] = 1\n",
    "player_data.loc[player_data[\"Position\"].isin(midfielder), \"Duty\"] = 2\n",
    "player_data.loc[player_data[\"Position\"].isin(forward), \"Duty\"] = 3\n",
    "\n",
    "player_data['Duty'] = player_data['Duty'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step three : filter out columns without ability info.\n",
    "ability = ['Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling',\n",
    "       'Curve', 'FKAccuracy', 'LongPassing', 'BallControl', 'Acceleration',\n",
    "       'SprintSpeed', 'Agility', 'Reactions', 'Balance', 'ShotPower',\n",
    "       'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression',\n",
    "       'Interceptions', 'Positioning', 'Vision', 'Penalties', 'Composure',\n",
    "       'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling',\n",
    "       'GKKicking', 'GKPositioning', 'GKReflexes', 'Duty']\n",
    "\n",
    "player_data_filtered = player_data[ability]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# fill the ability attribute of null with the mean_value of the column. \n",
    "# but in fact our dataset is complete enough with no null values now ! so there is no need to fill the null values.\n",
    "col_with_null = [col for col in player_data_filtered.columns\n",
    "                if player_data_filtered[col].isnull().any()]\n",
    "\n",
    "print(col_with_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "player_data_filtered.to_csv('/kaggle/working/filtered_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics about the filtered dataset.\n",
    "\n",
    "1. how many records & how many attributes are there in our filtered dataset.\n",
    "\n",
    "2. some statistic about each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(player_data_filtered.shape)\n",
    "\n",
    "player_data_filtered.describe().T.to_csv('/kaggle/working/description.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# standardlize the attributes of players.\n",
    "\n",
    "X = player_data_filtered.drop(\"Duty\", axis = 1)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "Scaler = StandardScaler()\n",
    "\n",
    "X = Scaler.fit_transform(X)\n",
    "\n",
    "Y = player_data_filtered['Duty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9b92edfe0a4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Step two : construct our dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPlayer_Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "#  Step one :split our traininng and test set.\n",
    "x_train = X[:14000]; y_train = Y[:14000]\n",
    "\n",
    "x_test = X[14000:]; y_test = Y[14000:]\n",
    "y_test.index = range(len(x_test))\n",
    "\n",
    "# Step two : construct our dataset.\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "class Player_Dataset(data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.input = x\n",
    "        self.target = y\n",
    "    def __getitem__(self, index):\n",
    "        return self.input[index], self.target[inex]\n",
    "    def  __len__(self):\n",
    "        return len(self.target)\n",
    "    \n",
    "train_dataset = Player_Dataset(x_train, y_train)\n",
    "\n",
    "test_dataset = Player_Dataset(x_test, y_test)\n",
    "\n",
    "# Step Three : construct our dataloader.\n",
    "train_iter = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "test_iter = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Model\n",
    "\n",
    "First Task is to use a Neural Network module to do a claasification on Duty according to the attributes of the player.\n",
    "\n",
    "The Second Task is to build a K-Means model to cluster the player ability attribute.\n",
    "\n",
    "The last task is to consider the Association Rules between Player's Age and Player's Potential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first Task \n",
    "\n",
    "Using pytorch to build a nerual network classifier to predict the player's duty.\n",
    "\n",
    "1. build our model \n",
    "\n",
    "2. define our optimizer and criterion\n",
    "\n",
    "3. train our model\n",
    "\n",
    "4. evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.output_size = output_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.linear_1 = nn.Linear(input_size, 128)\n",
    "        self.linear_2 = nn.Linear(128, 128)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_3 = nn.Linear(128, output_size)\n",
    "        \n",
    "    def forward(self, input):\n",
    "#         print(input.shape)\n",
    "        temp_result = F.relu(self.linear_1(input))\n",
    "#         print(temp_result.shape)\n",
    "        temp_result = F.relu(self.linear_2(temp_result))\n",
    "#         print(temp_result.shape)\n",
    "        temp_result = self.dropout(temp_result)\n",
    "#         print(temp_result.shape)\n",
    "        result = F.softmax(self.linear_3(temp_result), dim=1)\n",
    "#         print(result.shape)\n",
    "        return result\n",
    "    \n",
    "model = Classifier(33, 4, 0.5).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define our optimizer and  criterion.\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(data):\n",
    "        Input, target = batch\n",
    "        Input = Input.float().to(device); target = target.to(device)\n",
    "\n",
    "        output = model(Input)\n",
    "#         print(\"output\", output)\n",
    "#         print(\"target\", target)\n",
    "        loss = criterion(output, target)\n",
    "#         print(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time  / 60)\n",
    "    elapsed_secs = int(elapsed_time -  (elapsed_mins * 60))\n",
    "    return  elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_epoches = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_epoches):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iter, optimizer, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if train_loss < best_valid_loss:\n",
    "        best_valid_loss = train_loss\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "        \n",
    "    print(f'Epoch:  {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain  Loss: {train_loss: .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate():\n",
    "    result_on_test = []; ground_truth = []\n",
    "    model.eval()\n",
    "    for i, batch in enumerate(test_iter):\n",
    "        features, target = batch\n",
    "        ground_truth.extend(list(target))\n",
    "        with torch.no_grad():\n",
    "            output = model(features.float())\n",
    "            output = np.argmax(output, axis=1)\n",
    "            result_on_test.extend(output)\n",
    "            \n",
    "    acc = sum([(ground_truth[i] == result_on_test[i]).item() for i in range(len(result_on_test))]) / len(result_on_test)\n",
    "    print(f\"The classification accuracy is : {acc*100:.2f} %\\n\")   \n",
    "    print(classification_report(ground_truth, result_on_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task two \n",
    "\n",
    "we are going to build a K-Means model to do the cluster task on player's attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "# using sklearn create KMeans model to do our experiments.\n",
    "KMean_model = KMeans(n_clusters=4, random_state=9)\n",
    "y_pred = KMean_model.fit_predict(X)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "%pylab inline\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_decomposition = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.scatter(X_decomposition[:, 0], X_decomposition[:,1], c = y_pred,\n",
    "                 cmap = plt.cm.get_cmap(\"tab10\", 10), alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(metrics.calinski_harabasz_score(X, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(Y[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,10))    \n",
    "\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "# print(classification_report(list(Y), y_pred))\n",
    "\n",
    "plot_confusion_matrix(list(Y), y_pred, classes=np.array([\"0\",\"1\",\"2\",\"3\"]), title='Confusion matrix using KMeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
